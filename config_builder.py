import json
import os

from src.models.model_loaders import LIBRARY_LEARNER, SAMPLE_GENERATOR

DEFAULT_EXPERIMENT_DIR = "experiments_iterative"
DEFAULT_TEMPLATE_DIR = os.path.join(DEFAULT_EXPERIMENT_DIR, "templates")


DEFAULT_STITCH_PARAMS = {
    "max_arity": 2,
    "iterations": 1,
    "candidates_per_iteration": 1,
}

DEFAULT_CODEX_PARAMS = {
    "debug": False,
    "use_cached": False,
    "n_samples": 100,
    "n_train_programs_per_prompt": 25,
    "temperature": 0.75,
    "max_tokens": 256,
    "function_name_classes": ["default"],
}


def get_domain_metadata(domain: str):
    METADATA = {
        "logo": {
            "tasks_loader": "compositional_graphics_200",
            "task_language_loader": "compositional_graphics_200_synthetic",
            "ocaml_special_handler": "LOGO",
        },
        "clevr": {
            "tasks_loader": "clevr",
            "task_language_loader": "clevr_synthetic",
            "ocaml_special_handler": "clevr",
        },
        "re2": {
            "tasks_loader": "re2",
            "task_language_loader": "re2_synthetic",
            "ocaml_special_handler": "re2",
        },
    }
    return METADATA[domain]


def build_config(
    experiment_type: str,
    domain: str,
    experiment_name: str = None,
    output_directory: str = "experiments_iterative",
    random_seed: int = 0,
    max_iterations: int = 1,
    task_batcher: str = "ground_truth_ordered_task_batcher",
    global_batch_size: int = "all",
    stitch_params: dict = DEFAULT_STITCH_PARAMS,
    codex_params: dict = DEFAULT_CODEX_PARAMS,
):
    config = {}
    config.update(
        build_config_body(
            experiment_type=experiment_type,
            domain=domain,
            max_iterations=max_iterations,
            task_batcher=task_batcher,
            global_batch_size=global_batch_size,
            stitch_params=stitch_params,
            codex_params=codex_params,
        )
    )
    config.update(
        build_config_metadata(
            domain=domain,
            experiment_name=experiment_name,
            output_directory=output_directory,
            random_seed=random_seed,
        )
    )
    return config


def build_config_metadata(
    domain: str,
    experiment_name: str = None,
    output_directory: str = "experiments_iterative",
    random_seed: int = 0,
):
    meta = get_domain_metadata(domain)

    experiment_id = (
        f"{domain}_{experiment_name}" if experiment_name is not None else domain
    )

    return {
        "metadata": {
            "experiment_id": experiment_id,
            "human_readable": "Autogenerated iterative experiment.",
            "export_directory": f"{output_directory}/{experiment_id}/outputs",
            "log_directory": f"{output_directory}/{experiment_id}/logs",
            "tasks_loader": meta["tasks_loader"],
            "task_language_loader": meta["task_language_loader"],
            "export_with_timestamp": False,
            "resume_checkpoint_directory": None,
            "init_frontiers_from_checkpoint": False,
            "ocaml_special_handler": meta["ocaml_special_handler"],
            "random_seed": 0,
        }
    }


def build_config_body(
    experiment_type: str,
    domain: str,
    max_iterations: int = 1,
    task_batcher: str = "ground_truth_ordered_task_batcher",
    global_batch_size: int = "all",
    stitch_params: dict = DEFAULT_STITCH_PARAMS,
    codex_params: dict = DEFAULT_CODEX_PARAMS,
):
    template_path = os.path.join(
        DEFAULT_TEMPLATE_DIR, f"template_{experiment_type}.json"
    )
    with open(template_path, "r") as f:
        config = json.load(f)

    meta = get_domain_metadata(domain)

    model_initializers = config["model_initializers"]
    model_initializers[0]["model_loader"] = meta["ocaml_special_handler"]
    config["model_initializers"] = model_initializers

    config["experiment_iterator"]["max_iterations"] = max_iterations
    config["experiment_iterator"]["task_batcher"]["model_type"] = task_batcher
    config["experiment_iterator"]["task_batcher"]["params"][
        "global_batch_size"
    ] = global_batch_size

    loop_blocks = []
    for block in config["experiment_iterator"]["loop_blocks"]:
        if block.get("model_type") == SAMPLE_GENERATOR:
            block["params"].update(codex_params)
        if block.get("model_type") == LIBRARY_LEARNER:
            block["params"].update(stitch_params)
        loop_blocks.append(block)
    config["experiment_iterator"]["loop_blocks"] = loop_blocks

    return config
